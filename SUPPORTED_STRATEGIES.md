# LM Evaluation Harness - Supported Strategies Analysis

This document provides a comprehensive analysis of which strategies from the unified evaluation workflow are supported by the LM Evaluation Harness (lm-eval), classifying them into three categories:

## Classification Framework

### ‚úÖ Native Support (Fully Supported)
Strategies available immediately after installing the harness with:
- Only import statements and minimal configuration (‚â§2 lines)
- No external dependencies beyond the harness itself
- No custom implementation or glue code required

**Example:**
```python
from lm_eval import simple_evaluate
simple_evaluate(model="hf", tasks=["hellaswag"])
```

### üîå Integrated Support (Supported via Third-Party Integration)
Strategies requiring:
- Installation of ‚â•1 external package(s) (e.g., `pip install lm_eval[wandb]`)
- Glue code or configuration (typically ‚â§10 lines)
- Documented integration pattern or official example
- Functionality enabled through third-party tools

**Example:**
```python
# Requires: pip install lm_eval[wandb]
simple_evaluate(model="hf", tasks=["hellaswag"], 
                wandb_args={"project": "my-project"})
```

### ‚ùå Not Supported
Strategies requiring significant custom implementation or not supported at all.

---

**Quick Summary:** 
- **13 strategies natively supported** (out-of-the-box)
- **4 strategies supported via third-party integration** (with external packages)
- **17 strategies not supported**
- **Total: 34 strategies**

**Best Use Case:** Offline batch evaluation of language models on static benchmark datasets with traditional NLP metrics.

**Not Suitable For:** Production monitoring, interactive agents, performance benchmarking, non-LM systems.

---

## Table of Contents

- [Classification Framework](#classification-framework)
- [Phase 0: Provisioning (The Runtime)](#phase-0-provisioning-the-runtime) - 4 native, 0 integrated, 4 not supported
- [Phase I: Specification (The Contract)](#phase-i-specification-the-contract) - 4 native, 1 integrated, 5 not supported
- [Phase II: Execution (The Run)](#phase-ii-execution-the-run) - 1 native, 0 integrated, 3 not supported
- [Phase III: Assessment (The Score)](#phase-iii-assessment-the-score) - 3 native, 2 integrated, 1 not supported
- [Phase IV: Reporting (The Output)](#phase-iv-reporting-the-output) - 1 native, 1 integrated, 4 not supported
- [Summary Statistics](#summary-statistics)
- [Conclusion](#conclusion)

---

## Phase 0: Provisioning (The Runtime)

### Step A: Harness Installation

#### ‚úÖ Strategy 1: PyPI Packages
**Status:** NATIVELY SUPPORTED

The harness is primarily distributed via PyPI and can be installed using pip:
```bash
pip install lm_eval
pip install lm_eval[hf]  # For HuggingFace models
pip install lm_eval[vllm]  # For vLLM inference
```

**Evidence:**
- README.md lines 58-95: Documents pip installation with optional extras
- pyproject.toml: Defines package structure and optional dependencies
- Multiple optional extras for different backends (hf, vllm, api, etc.)

**Classification:** Native - requires only pip install command, no custom code needed.

---

#### ‚úÖ Strategy 2: Git Clone
**Status:** NATIVELY SUPPORTED

The harness can be cloned and installed from source:
```bash
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
```

**Evidence:**
- README.md lines 60-66: Documents git clone installation
- CONTRIBUTING.md references development setup via git clone

**Classification:** Native - standard git clone + pip install workflow, no custom code needed.

---

#### ‚ùå Strategy 3: Container Images
**Status:** NOT SUPPORTED

No prebuilt Docker or OCI container images are provided by the harness.

**Evidence:**
- No Dockerfile in repository root
- No documentation of container images
- No references to Docker Hub or container registries

---

#### ‚ùå Strategy 4: Binary Packages
**Status:** NOT SUPPORTED

No standalone executable binaries are provided.

**Evidence:**
- Package is distributed as Python source/wheel only
- No binary distribution mechanism documented

---

#### ‚ùå Strategy 5: Node Package
**Status:** NOT SUPPORTED

This is a Python-based harness, not a JavaScript/Node.js package.

**Evidence:**
- Pure Python implementation
- No package.json or npm configuration

---

### Step B: Service Authentication

#### ‚ùå Strategy 1: Evaluation Platform Authentication
**Status:** NOT SUPPORTED

The harness does not provide native authentication with evaluation platforms or leaderboard submission APIs.

**Evidence:**
- No CLI commands for platform authentication
- No built-in leaderboard submission functionality
- HuggingFace Hub integration exists but only for logging results, not for leaderboard submission

---

#### ‚úÖ Strategy 2: API Provider Authentication
**Status:** NATIVELY SUPPORTED

The harness supports authentication with commercial model providers via environment variables and API keys.

**Evidence:**
- README.md lines 399-403: Documents OpenAI API key usage
- API_guide.md lines 151-158: Shows API key retrieval pattern
- models/openai_completions.py: Implements API key handling
- models/anthropic_llms.py: Implements Anthropic API authentication
- models/ibm_watsonx_ai.py: Implements Watson AI authentication

**Supported providers:**
- OpenAI (OPENAI_API_KEY)
- Anthropic (ANTHROPIC_API_KEY)
- TextSynth
- IBM Watsonx.ai

**Classification:** Native - requires only setting environment variables (1 line: `export OPENAI_API_KEY=...`), no glue code needed.

---

#### ‚úÖ Strategy 3: Repository Authentication
**Status:** NATIVELY SUPPORTED

The harness supports authentication with HuggingFace Hub for accessing gated models and datasets.

**Evidence:**
- interface.md line 264: Documents HF_TOKEN environment variable
- README.md: References authentication for private datasets/models
- Uses HuggingFace datasets library which supports authentication

**Classification:** Native - requires only setting HF_TOKEN environment variable (1 line), no glue code needed.

---

## Phase I: Specification (The Contract)

### Step A: SUT Preparation

#### ‚úÖ Strategy 1: Model-as-a-Service (Remote Inference)
**Status:** NATIVELY SUPPORTED

The harness supports remote inference via multiple API providers and self-hosted servers.

**Evidence:**
- README.md lines 389-437: Extensive documentation of API model support
- Supported APIs include:
  - OpenAI Completions and ChatCompletions
  - Anthropic
  - TextSynth
  - IBM Watsonx.ai
  - Local completions servers (OpenAI-compatible)
- models/openai_completions.py: Implementation
- models/anthropic_llms.py: Implementation
- models/api_models.py: Template API implementation

**Classification:** Native - after `pip install lm_eval[api]` and setting API key, requires only 1-2 lines:
```python
simple_evaluate(model="openai-completions", model_args="model=gpt-3.5-turbo", tasks=["hellaswag"])
```

---

#### ‚úÖ Strategy 2: Model-in-Process (Local Inference)
**Status:** NATIVELY SUPPORTED

The harness extensively supports loading and running models locally.

**Evidence:**
- README.md lines 117-232: HuggingFace transformers integration
- README.md lines 278-327: NeMo models
- README.md lines 343-362: vLLM support
- README.md lines 364-388: SGLang support
- README.md lines 421-428: Multiple model backends

**Supported backends:**
- HuggingFace Transformers (AutoModelForCausalLM, AutoModelForSeq2SeqLM)
- vLLM
- SGLang
- Mamba SSM
- NVIDIA NeMo
- OpenVINO (Optimum)
- Neuron (AWS Inferentia)
- GGUF/llama.cpp
- Quantized models (GPTQ, GPTQModel)

**Classification:** Native - after `pip install lm_eval[hf]`, requires only 1-2 lines:
```python
simple_evaluate(model="hf", model_args="pretrained=gpt2", tasks=["hellaswag"])
```

---

#### ‚ùå Strategy 3: Algorithm Implementation (In-Memory Structures)
**Status:** NOT SUPPORTED

The harness does not natively support ANN algorithms, knowledge graph embeddings, or BM25 indexes.

**Evidence:**
- Focus is exclusively on neural language models
- No support for vector indexes (FAISS, HNSW)
- No support for ranking algorithms like BM25
- No graph embedding support

---

#### ‚ùå Strategy 4: Policy/Agent Instantiation (Stateful Controllers)
**Status:** NOT SUPPORTED

The harness does not natively support RL policies or autonomous agents as primary SUTs.

**Evidence:**
- Model interface (model_guide.md lines 33-47) only supports:
  - loglikelihood
  - loglikelihood_rolling
  - generate_until
- No support for RL policy evaluation
- No environment stepping interface
- No multi-agent coordination

**Note:** While tasks can evaluate agent-like behavior via generate_until, the harness does not support instantiating agents as stateful controllers.

---

### Step B: Benchmark Preparation (Inputs)

#### ‚úÖ Strategy 1: Benchmark Dataset Preparation (Offline)
**Status:** NATIVELY SUPPORTED

The harness has extensive support for loading and preparing benchmark datasets.

**Evidence:**
- task_guide.md lines 24-34: Dataset configuration options
- README.md line 47: "Over 60 standard academic benchmarks"
- Tasks use HuggingFace datasets library
- Support for:
  - Remote dataset loading from HF Hub
  - Local dataset loading (JSON, CSV)
  - Data splitting (train/validation/test/fewshot)
  - Preprocessing via `process_docs`
  - Dataset transformations

**Configuration options:**
- dataset_path
- dataset_name
- dataset_kwargs
- training_split, validation_split, test_split, fewshot_split
- process_docs function

**Classification:** Native - datasets loaded automatically via task YAML configs, requires only:
```python
simple_evaluate(model="hf", model_args="pretrained=gpt2", tasks=["hellaswag"])
```

---

#### ‚ùå Strategy 2: Synthetic Data Generation (Generative)
**Status:** NOT SUPPORTED

The harness does not natively generate synthetic test data.

**Evidence:**
- No built-in data augmentation or perturbation
- No synthetic trajectory generation
- Tasks load pre-existing datasets only
- No generative data creation utilities

**Note:** While tasks can be configured to use custom datasets (custom_dataset callable), the harness itself provides no synthetic generation capabilities.

---

#### ‚ùå Strategy 3: Simulation Environment Setup (Simulated)
**Status:** NOT SUPPORTED

The harness does not support interactive simulation environments.

**Evidence:**
- No 3D environment support
- No scene construction capabilities
- No physics simulation
- Focus is on static datasets, not dynamic environments

---

#### ‚ùå Strategy 4: Production Traffic Sampling (Online)
**Status:** NOT SUPPORTED

The harness does not support sampling real-world inference traffic.

**Evidence:**
- Evaluation is offline/batch-based
- No streaming traffic capture
- No production monitoring integration
- No online sampling mechanism

---

### Step C: Benchmark Preparation (References)

#### üîå Strategy 1: Judge Preparation
**Status:** SUPPORTED VIA THIRD-PARTY INTEGRATION

The harness supports using LLM judges and pre-trained evaluator models through custom metric functions that integrate with external models or libraries.

**Evidence:**
- task_guide.md: Supports embedding Python functions for metrics
- Tasks can use LLM-based evaluation via custom functions
- Support for model-based metrics through extensible metric system
- Can call external models as judges

**Integration approach:**
- Requires custom metric functions (5-10 lines of glue code)
- Can integrate with external LLMs via API calls
- Can use HuggingFace Evaluate library metrics
- Python function embedding (!function directive) for custom judges

**Classification:** Integrated - requires custom metric function implementation and potentially external dependencies (e.g., calling OpenAI for LLM-as-judge):
```python
# Requires custom metric function + external LLM call
def llm_judge(predictions, references):
    # 5-10 lines of glue code to call external LLM
    return scores
```

---

#### ‚úÖ Strategy 2: Ground Truth Preparation
**Status:** NATIVELY SUPPORTED

The harness supports loading and using ground truth references from datasets.

**Evidence:**
- task_guide.md lines 40-41: doc_to_target for ground truth
- Datasets include reference answers
- Support for multiple-choice answer keys
- Human annotations loaded from datasets
- Reference-based metrics (BLEU, ROUGE, exact match)

**Classification:** Native - ground truth loaded automatically from dataset configurations, no custom code needed.

---

## Phase II: Execution (The Run)

### Step A: SUT Invocation

#### ‚úÖ Strategy 1: Batch Inference
**Status:** NATIVELY SUPPORTED

The harness is designed around batch inference as its primary execution mode.

**Evidence:**
- README.md: All examples show batch evaluation
- interface.md line 90: --batch_size parameter
- Supports auto batch sizing
- Processes multiple samples through a single model instance
- evaluator.py: Core evaluation loop processes batches

**Features:**
- Fixed batch sizes
- Auto batch size detection
- Multi-GPU data parallelism
- Reordering by length for efficiency

**Classification:** Native - batch inference is the default mode, requires only:
```python
simple_evaluate(model="hf", model_args="pretrained=gpt2", tasks=["hellaswag"], batch_size=8)
```

---

#### ‚ùå Strategy 2: Interactive Loop
**Status:** NOT SUPPORTED (NATIVELY)

The harness does not natively support stateful environment stepping.

**Evidence:**
- Model interface is stateless (generate_until, loglikelihood)
- No native environment stepping API
- No trajectory rollout support
- Tasks evaluate single-turn or multi-turn conversations, but not stateful RL-style interactions

**Note:** While generate_until can be used iteratively, there's no native support for $(State_t, Action_t) \rightarrow State_{t+1}$ transitions with environment feedback.

---

#### ‚ùå Strategy 3: Arena Battle
**Status:** NOT SUPPORTED

The harness does not natively support pairwise model comparison on the same inputs.

**Evidence:**
- Evaluation runs process one model at a time
- No built-in arena comparison mode
- Would require running separate evaluations and comparing results manually
- No pairwise comparison infrastructure

---

#### ‚ùå Strategy 4: Production Streaming
**Status:** NOT SUPPORTED

The harness does not support continuous production traffic processing.

**Evidence:**
- Offline evaluation only
- No streaming traffic support
- No real-time metric collection
- No production deployment integration

---

## Phase III: Assessment (The Score)

### Step A: Individual Scoring

#### ‚úÖ Strategy 1: Deterministic Measurement
**Status:** NATIVELY SUPPORTED

The harness extensively supports deterministic metrics.

**Evidence:**
- task_guide.md lines 223-237: Native metrics list
- Supported metrics include:
  - Accuracy (acc, acc_norm)
  - Exact match
  - F1 score
  - BLEU
  - ROUGE
  - Perplexity
  - Matthews correlation coefficient

**Implementation:**
- lm_eval/api/metrics.py: Core metrics
- Built-in aggregation functions
- Support for custom metrics via @register_metric

**Classification:** Native - metrics available out-of-the-box, automatically computed based on task configuration.

---

#### üîå Strategy 2: Embedding Measurement
**Status:** SUPPORTED VIA THIRD-PARTY INTEGRATION

The harness supports embedding-based metrics through integration with external libraries like HuggingFace Evaluate.

**Evidence:**
- task_guide.md: Allows custom metric functions
- Tasks can use embedding-based metrics via custom implementations
- Support for importing external metrics from HuggingFace Evaluate library
- Can implement BERTScore, sentence embeddings, etc. via custom metrics

**Integration approach:**
- Requires installing external libraries (e.g., `pip install bert-score` or `pip install evaluate`)
- Custom metric functions (5-10 lines of glue code)
- Python function embedding (!function directive)

**Classification:** Integrated - requires external packages and glue code:
```python
# Requires: pip install evaluate
# Plus 5-10 lines in custom metric function
from evaluate import load
bertscore = load("bertscore")

def custom_bertscore(predictions, references):
    results = bertscore.compute(predictions=predictions, references=references, lang="en")
    return results["f1"]
```

**Note:** While BERTScore and similar metrics are not built-in, they can be integrated via documented patterns.

---

#### üîå Strategy 3: Subjective Measurement
**Status:** SUPPORTED VIA THIRD-PARTY INTEGRATION

The harness supports model-based and LLM-based evaluation through custom metric functions that call external models.

**Evidence:**
- task_guide.md: Custom metric functions can call LLMs
- Tasks can implement LLM-as-judge evaluation
- Support for calling external models for assessment via API
- Extensible metric system allows subjective evaluation

**Integration approach:**
- Requires external LLM API access (OpenAI, Anthropic, etc.)
- Custom metric functions (5-10 lines of glue code)
- Flexible Python function support

**Classification:** Integrated - requires external API access and custom implementation:
```python
# Requires: API key set + custom metric function
import openai

def llm_judge(predictions, references):
    # 5-10 lines to call external LLM for judgment
    response = openai.ChatCompletion.create(...)
    return parse_score(response)
```

---

#### ‚ùå Strategy 4: Performance Measurement
**Status:** NOT SUPPORTED (NATIVELY)

The harness does not natively measure latency, throughput, memory, or energy consumption.

**Evidence:**
- Focus is on quality metrics, not performance metrics
- No built-in latency measurement
- No throughput tracking
- No memory profiling
- No energy/carbon footprint measurement

**Note:** Users can add timing wrappers around evaluations, but this is not a native feature.

---

### Step B: Collective Aggregation

#### ‚úÖ Strategy 1: Score Aggregation
**Status:** NATIVELY SUPPORTED

The harness provides comprehensive score aggregation capabilities.

**Evidence:**
- task_guide.md lines 240-245: Aggregation functions
- lm_eval/api/metrics.py: Aggregation implementations
- task_guide.md lines 330-335: Group-level aggregation

**Supported aggregation:**
- Mean, median
- Weighted aggregation (micro/macro averaging)
- Perplexity aggregation
- Custom aggregation functions
- Group-level metric aggregation

**Classification:** Native - aggregation performed automatically based on metric definitions, no custom code needed.
- Weighted aggregation (micro/macro averaging)
- Perplexity aggregation
- Custom aggregation functions
- Group-level metric aggregation

---

#### ‚úÖ Strategy 2: Uncertainty Quantification
**Status:** NATIVELY SUPPORTED

The harness provides bootstrap resampling for standard error estimation.

**Evidence:**
- lm_eval/api/metrics.py lines 516-580: bootstrap_stderr implementation
- Supports bootstrap resampling for approved metrics (median, matthews_corrcoef, f1_score, perplexity, BLEU, etc.)
- Parallel execution with multiprocessing
- Configurable bootstrap iterations
- Closed-form standard error for some metrics (mean, acc_all)

**Limitations:**
- No PPI (Prediction-Powered Inference) support
- Bootstrap only for pre-approved metric list
- No arbitrary confidence interval calculation beyond stderr

**Classification:** Native - bootstrap stderr computed automatically for supported metrics, no additional code needed.

---

## Phase IV: Reporting (The Output)

### Step A: Insight Presentation

#### ‚ùå Strategy 1: Execution Tracing
**Status:** NOT SUPPORTED

The harness does not display step-by-step execution logs for agent reasoning.

**Evidence:**
- --log_samples saves inputs/outputs but not intermediate steps
- No tool call tracing
- No reasoning state visualization
- Logs are for final outputs, not intermediate steps

---

#### ‚ùå Strategy 2: Subgroup Analysis
**Status:** NOT SUPPORTED

The harness does not provide native subgroup performance breakdown.

**Evidence:**
- Results are aggregated at task/subtask level
- No demographic stratification
- No built-in domain-wise breakdown beyond task groups
- Custom analysis would require post-processing results

---

#### ‚ùå Strategy 3: Chart Generation
**Status:** NOT SUPPORTED

The harness does not generate charts or visualizations natively.

**Evidence:**
- Output is JSON/text only
- No built-in plotting
- No chart generation utilities
- Visualization requires external tools

**Note:** Integration with Weights & Biases and Zeno provides visualization, but these are external dependencies, not native features.

---

#### üîå Strategy 4: Dashboard Creation
**Status:** SUPPORTED VIA THIRD-PARTY INTEGRATION

The harness supports dashboard creation through official integrations with Weights & Biases and Zeno.

**Evidence:**
- README.md lines 540-619: Weights & Biases integration
- README.md lines 544-584: Zeno visualization
- wandb_logger.py: W&B integration implementation
- pyproject.toml: Optional dependencies `lm_eval[wandb]` and `lm_eval[zeno]`

**Features (via W&B):**
- Interactive web interface
- Metric comparisons
- Result tables
- Run tracking

**Integration approach:**
- Requires: `pip install lm_eval[wandb]` or `pip install lm_eval[zeno]`
- W&B account setup and authentication
- Configuration via wandb_args parameter (1-2 lines)

**Classification:** Integrated - requires external package and service authentication:
```python
# Requires: pip install lm_eval[wandb] + wandb login
simple_evaluate(
    model="hf", 
    model_args="pretrained=gpt2",
    tasks=["hellaswag"],
    wandb_args={"project": "my-project", "name": "run-1"}
)
```

---

#### ‚úÖ Strategy 5: Leaderboard Publication
**Status:** NATIVELY SUPPORTED

The harness can publish results to HuggingFace Hub datasets for leaderboard use.

**Evidence:**
- README.md lines 519-536: HF Hub logging
- interface.md lines 153-165: hf_hub_log_args documentation
- README.md line 56: Backend for HF Open LLM Leaderboard

**Features:**
- Push results to HF Hub
- Push samples to HF Hub
- Leaderboard URL association
- Gated dataset support

**Classification:** Native - HF Hub integration is built-in, requires only configuration (‚â§2 lines):
```python
simple_evaluate(
    model="hf", model_args="pretrained=gpt2", tasks=["hellaswag"],
    hf_hub_log_args={"hub_results_org": "my-org", "push_results_to_hub": True}
)
```

**Note:** The harness is the official backend for HuggingFace's Open LLM Leaderboard.

---

#### ‚ùå Strategy 6: Regression Alerting
**Status:** NOT SUPPORTED

The harness does not provide automatic regression detection or alerting.

**Evidence:**
- No historical baseline comparison
- No automated alerting
- No performance degradation detection
- Results comparison would be manual

---

## Summary Statistics

### Overall Support by Phase

| Phase | Native | Integrated | Not Supported | Total |
|-------|--------|------------|---------------|-------|
| **Phase 0: Provisioning** | 4 | 0 | 4 | 8 |
| **Phase I: Specification** | 4 | 1 | 5 | 10 |
| **Phase II: Execution** | 1 | 0 | 3 | 4 |
| **Phase III: Assessment** | 3 | 2 | 1 | 6 |
| **Phase IV: Reporting** | 1 | 1 | 4 | 6 |
| **TOTAL** | **13** | **4** | **17** | **34** |

### Support Breakdown

**‚úÖ Natively Supported (13 strategies):**
1. **PyPI/Git Installation** - Standard Python package distribution
2. **API Provider Authentication** - Environment variable configuration
3. **Repository Authentication** - HF_TOKEN for gated resources
4. **Model-as-a-Service** - Built-in API model backends (OpenAI, Anthropic, etc.)
5. **Model-in-Process** - Native support for HF, vLLM, SGLang, NeMo, etc.
6. **Benchmark Dataset Preparation** - Automatic loading via HF datasets
7. **Ground Truth Preparation** - Loaded from dataset configurations
8. **Batch Inference** - Default execution mode
9. **Deterministic Metrics** - Built-in accuracy, BLEU, ROUGE, F1, etc.
10. **Score Aggregation** - Automatic aggregation across tasks
11. **Uncertainty Quantification** - Bootstrap stderr for approved metrics
12. **Leaderboard Publication** - Built-in HF Hub integration

**üîå Supported via Third-Party Integration (4 strategies):**
1. **Judge Preparation** (Phase I) - Custom metric functions calling external LLMs (5-10 lines glue code)
2. **Embedding Measurement** (Phase III) - Via HF Evaluate library + custom metrics (requires `pip install evaluate`)
3. **Subjective Measurement** (Phase III) - Custom metrics with LLM API calls (requires API access)
4. **Dashboard Creation** (Phase IV) - Via W&B or Zeno (requires `pip install lm_eval[wandb]` or `lm_eval[zeno]`)

**‚ùå Not Supported (17 strategies):**
1. **Containers & Binaries** - No prebuilt distributions
2. **Node Package** - Python-only
3. **Platform Authentication** - No evaluation platform APIs
4. **Non-LM SUTs** - No ANN, graph, or RL policy support
5. **Synthetic Data** - No generative test creation
6. **Environments** - No simulation or interactive environments
7. **Production Traffic** - No online sampling
8. **Interactive Loop** - No stateful stepping
9. **Arena Battle** - No pairwise comparison
10. **Production Streaming** - No real-time evaluation
11. **Performance Metrics** - No latency/throughput measurement
12. **Execution Tracing** - No step-by-step logs
13. **Subgroup Analysis** - No demographic stratification
14. **Chart Generation** - No native plotting
15. **Regression Alerting** - No automated monitoring
16. **Algorithm Implementation** - No ANN/graph/BM25 support
17. **Policy/Agent Instantiation** - No RL policies

---

## Conclusion

The **LM Evaluation Harness** is purpose-built for **offline batch evaluation of language models** on static benchmark datasets.

### Natively Supported (13/34 = 38%)

The harness excels at out-of-the-box support for:
- **Installation & Setup**: PyPI/Git distribution, API/repository authentication
- **Model Loading**: Extensive backends (HF Transformers, vLLM, SGLang, OpenAI, Anthropic, NeMo, etc.)
- **Dataset Management**: HuggingFace datasets integration with automatic loading
- **Batch Evaluation**: Core execution paradigm with auto batch sizing
- **Traditional Metrics**: Comprehensive deterministic metrics (accuracy, BLEU, ROUGE, F1, perplexity, etc.)
- **Statistical Analysis**: Bootstrap uncertainty quantification and multi-level aggregation
- **Result Publishing**: HuggingFace Hub integration for leaderboards

### Supported via Integration (4/34 = 12%)

Additional capabilities available through documented third-party integrations:
- **Advanced Evaluation**: LLM judges, embedding-based metrics (BERTScore), subjective assessment
- **Visualization**: Interactive dashboards via Weights & Biases or Zeno

**Total Supported: 17/34 (50%)**

### Not Supported (17/34 = 50%)

The harness is **NOT** designed for:
- **Production Monitoring**: No real-time streaming, regression alerting, or performance metrics
- **Interactive Evaluation**: No stateful environment stepping, arena battles, or RL policy evaluation
- **Alternative Deployment**: No container images, binaries, or Node packages
- **Non-LM Systems**: No support for ANN indexes, graph embeddings, or specialized ML algorithms
- **Dynamic Testing**: No synthetic data generation or simulation environments
- **Advanced Analytics**: No execution tracing, subgroup analysis, or native chart generation

### Recommendation

This focused scope makes **lm-eval excellent for academic benchmarking and offline model comparison**, providing robust native support for traditional NLP evaluation workflows. For production ML systems requiring real-time monitoring, interactive agents, or performance benchmarking, the harness would need significant extension or integration with complementary tools.
